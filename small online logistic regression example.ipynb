{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e182ece",
   "metadata": {},
   "source": [
    "# Description\n",
    "In this example I evolve an optimizer on a single logistic regression dataset with batch size of 1. I set number of steps to 100 because otherwise it is painfully slow (even though it's as fast as it can be). Realistically one should at least increase number of steps to 1000 to 10,000, or even better, evaluate on multiple datasets. But this might work too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myai.torch_tools import performance_tweaks\n",
    "performance_tweaks(True, deterministic=None)\n",
    "\n",
    "import torch\n",
    "import visualbench as vb\n",
    "from evolving_optimizers import *\n",
    "from evolving_optimizers._tasks import optimizer\n",
    "import numpy as np\n",
    "import joblib\n",
    "import copy\n",
    "import torchzero as tz\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "bench = vb.Collinear(vb.models.MLP([32, 10]), batch_size=1).to(DEVICE).set_performance_mode(True).set_print_inverval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_operands = (optimizer.Grad, optimizer.GradAt, optimizer.DirectionalGrad, optimizer.Hessp, optimizer.HesspAt, optimizer.DirectionalHessp)\n",
    "\n",
    "def evaluate_tree(tree: BaseOperation, print_progress=True):\n",
    "    operands = tree.flat_branches()\n",
    "    if len(operands) > 64:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    if not any(type(o) in optimizer_operands for o in operands):\n",
    "        return float(\"inf\")\n",
    "\n",
    "    opt_fn = lambda p,lr: optimizer.TreeOpt(p, lr, tree)\n",
    "    bench.tune(\n",
    "        opt_fn, grid=[-4, -2, 0], metrics=\"test loss\", log_scale=True, step=1,\n",
    "        num_candidates=1, num_binary=3, num_expansions=3,\n",
    "        max_passes=200, max_steps=100, print_progress=print_progress,\n",
    "    )\n",
    "\n",
    "    penalty = 0\n",
    "    if len(tree.flat_branches()) > 32:\n",
    "        penalty = (len(tree.flat_branches()) - 32) * 0.1\n",
    "\n",
    "    test_loss = bench.logger.last(\"test loss\")\n",
    "\n",
    "    if not math.isfinite(test_loss):\n",
    "        return float(\"inf\")\n",
    "\n",
    "    return float(test_loss + penalty)\n",
    "\n",
    "bench._test_epoch()\n",
    "print(f\"{bench.logger.min('test loss') = }\")\n",
    "for opt in reversed(optimizer.init_population()):\n",
    "    print(f\"{evaluate_tree(opt, True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fae75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner()\n",
    "pool = RandomPool(COMMON_POOL + optimizer.OPTIMIZER_POOL)\n",
    "\n",
    "opt = IslandGA([K1Plus1(), K1Plus1(), K1Plus1(), K1Plus1(), K1Plus1()], random_init=100)\n",
    "# opt = K1Plus1()\n",
    "# opt = ARS(0.9)\n",
    "\n",
    "with DiskCachedObjective(\"evaluated.log\", map_objective(evaluate_tree)) as objective:\n",
    "    population = objective(optimizer.init_population())\n",
    "    lowest_loss = float(\"inf\")\n",
    "\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "\n",
    "            population = runner.step(objective, opt, population, pool)\n",
    "\n",
    "            if runner.best_solutions[0].fitness < lowest_loss:\n",
    "                print(f'{i}: {runner.best_solutions[0].fitness:.3f}, {runner.best_solutions[0].tree}')\n",
    "                lowest_loss = runner.best_solutions[0].fitness\n",
    "            else:\n",
    "                print(f'{i}', end=\"\\r\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e99d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in population:\n",
    "    print(ind.fitness, len(ind.tree.flat_branches()), ind.tree.string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69bff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in runner.best_solutions:\n",
    "    print(ind.fitness, len(ind.tree.flat_branches()), ind.tree.string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fa94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = vb.FunctionDescent(\"booth\")#.set_noise(10,1)\n",
    "opt = optimizer.TreeOpt(bench.parameters(), 1e-1, runner.best_solutions[0].tree)\n",
    "bench.run(opt, 1000).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec13dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.render(\"logreg1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626480f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(runner.best_solutions[0].tree, f\"logreg1 {lowest_loss:.5f} K1Plus1.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
